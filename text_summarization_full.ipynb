{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (3.7.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (2.28.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (1.10.15)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: jinja2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/akshay/.local/lib/python3.7/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (4.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /home/akshay/.local/lib/python3.7/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.10.0,>=0.3.0->spacy) (4.11.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 8.4 MB/s eta 0:00:01    |████████████████▍               | 6.5 MB 8.4 MB/s eta 0:00:01     |████████████████████▏           | 8.1 MB 8.4 MB/s eta 0:00:01     |██████████████████████▋         | 9.1 MB 8.4 MB/s eta 0:00:01     |████████████████████████▌       | 9.8 MB 8.4 MB/s eta 0:00:01     |████████████████████████████▌   | 11.4 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/akshay/.local/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.21.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.15)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: jinja2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.0)\n",
      "Requirement already satisfied: setuptools in /home/akshay/.local/lib/python3.7/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: nltk in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: joblib in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from click->nltk) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.4.0)\n",
      "/home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /home/akshay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install nltk\n",
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'akshay']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    " \n",
    "#the stemmer requires a language parameter\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "word_tokenize(\"I am akshay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRT = \"<\\start>\"\n",
    "END = \"<\\end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my', 'both', 'everything', 'there', 'ca', 'elsewhere', 'show', 'whoever', 'therein', 'get', 'whence', 'others', 'hereafter', 'any', 'whenever', 'former', 'back', 'due', 'except', 'whose', 'quite', 'unless', 'into', 'those', 'anyway', 'other', 'indeed', 'name', 'say', \"'d\", 'side', 'make', 'under', 'third', 'at', 'using', 'how', '’s', 'why', 'next', 'before', 'made', 'i', 'wherein', 'than', 'noone', 'could', 'below', 'twelve', 'please', 'ten', 'much', 'three', 'amongst', 'against', 'often', 'sometime', 'themselves', 'give', 'were', 'most', 'twenty', 'neither', 'well', 'yourselves', 'out', 'along', 'they', 'also', 'becoming', 'wherever', 'amount', 'whereupon', 'though', 'when', 'your', 'herself', 'us', 'otherwise', 'him', 'up', 'none', 'ours', 'doing', 'on', 'one', 'all', 'so', 'without', 'it', 'yourself', 'if', 'just', '’ll', 'take', 'would', 'of', 'part', 'while', 'thence', 'already', 'them', 'never', 'a', '‘re', 'seemed', 'various', 'have', 'keep', \"n't\", 'n’t', 'call', 'these', 'upon', 'beyond', 'became', 'that', 'around', 'toward', 'then', '‘d', \"'ve\", 'off', 'does', \"'ll\", 'himself', 'somewhere', 'after', 'thereby', 'namely', 'each', 'now', 'anywhere', 'seeming', 'therefore', 'per', 'anything', 'across', 'nobody', 'moreover', 'over', 'she', 'hereupon', 'put', 'last', 'becomes', 'where', 'further', 'no', 'did', 'should', 'is', 'besides', 'once', 'anyone', \"'re\", 'here', 'whereafter', 'n‘t', 'and', 'become', 'nine', 'first', 'since', 'serious', 'above', 'be', 'its', '’ve', 'as', 'which', 'mine', 'move', 'not', 'among', 'afterwards', 'seems', 'sometimes', 'still', 'are', 'ourselves', 'fifteen', 'am', 'whereas', 'two', 'been', 'same', 'forty', 'might', 'thus', 'done', 'empty', 'eight', 'else', 'must', 'with', 'itself', 'only', 'someone', 'cannot', 'four', 'alone', 'front', 'throughout', 'several', 'bottom', 'herein', 'least', 'anyhow', '‘m', '‘s', \"'m\", 'mostly', 'may', 'more', 'again', 'another', 'formerly', 'nothing', 'until', 'eleven', 'through', 'whether', 'will', 'hundred', 'to', 'whither', 'their', 'five', 'because', 'every', 'via', 'almost', 'everyone', 'full', 'during', 'seem', 'either', 'rather', 'go', 'few', 'enough', '’m', 'our', 'this', 'onto', 'whereby', 'used', 'about', 'within', 'sixty', 'for', 'me', 'own', 'was', 'behind', 'who', 'we', 'hence', 'his', 'something', 'latterly', \"'s\", 'yours', '‘ve', 'whom', 'top', 'an', 'however', '‘ll', 'such', '’d', 'less', 'nevertheless', 'regarding', 'very', 'had', 'in', 're', 'perhaps', 'thereafter', 'between', '’re', 'nowhere', 'always', 'yet', 'beforehand', 'hereby', 'see', 'the', 'her', 'thereupon', 'towards', 'some', 'you', 'what', 'has', 'whatever', 'nor', 'everywhere', 'hers', 'many', 'really', 'thru', 'fifty', 'down', 'too', 'can', 'somehow', 'meanwhile', 'ever', 'although', 'beside', 'together', 'whole', 'latter', 'but', 'do', 'even', 'by', 'or', 'myself', 'he', 'being', 'from', 'six'}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "punctuations = string.punctuation\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_200_wv = api.load('glove-twitter-200')\n",
    "gnews_300_wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_wordvector(wv,keys):\n",
    "    cnt = 0\n",
    "    mean_wv = None\n",
    "    for k in keys:\n",
    "        cv = wv[k]\n",
    "        if mean_wv is None:\n",
    "            mean_wv = cv\n",
    "        else:\n",
    "            mean_wv = (mean_wv * (cnt-1) + cv)/cnt\n",
    "        cnt+=1\n",
    "    \n",
    "    return mean_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_rep_glove_twitter_200 = mean_wordvector(glove_twitter_200_wv,glove_twitter_200_wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_rep_gnews_300 = mean_wordvector(gnews_300_wv,gnews_300_wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('england', 0.6504806280136108),\n",
       "  ('rugby', 0.6489739418029785),\n",
       "  ('lanka', 0.6265019774436951),\n",
       "  ('ipl', 0.6233053803443909),\n",
       "  ('sachin', 0.6209328770637512),\n",
       "  ('odi', 0.6159676313400269),\n",
       "  ('football', 0.6020534038543701),\n",
       "  ('batting', 0.601995587348938),\n",
       "  ('pakistan', 0.597729504108429),\n",
       "  ('footy', 0.5967651605606079)],\n",
       " [('cricketing', 0.8372225761413574),\n",
       "  ('cricketers', 0.8165745735168457),\n",
       "  ('Test_cricket', 0.8094819784164429),\n",
       "  ('Twenty##_cricket', 0.8068488240242004),\n",
       "  ('Twenty##', 0.7624265551567078),\n",
       "  ('Cricket', 0.7541396617889404),\n",
       "  ('cricketer', 0.7372578382492065),\n",
       "  ('twenty##', 0.7316356897354126),\n",
       "  ('T##_cricket', 0.7304614186286926),\n",
       "  ('West_Indies_cricket', 0.698798656463623)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_twitter_200_wv.most_similar(\"cricket\"),gnews_300_wv.most_similar(\"cricket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatizer(sentence,is_remove_stopword=False,is_remove_punct=False,verbose=False):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "    if(verbose):\n",
    "        print(mytokens)\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if (not is_remove_stopword or word not in stop_words) and (not is_remove_punct or word not in punctuations) ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_stemmer(stemmer_obj,sentence,is_remove_stopword=False,is_remove_punct=False):\n",
    "    mytokens = word_tokenize(sentence)\n",
    "    # print(mytokens)\n",
    "    # Removing stop words and punctuations\n",
    "    mytokens = [ stemmer_obj.stem(word) for word in mytokens if (not is_remove_stopword or word not in stop_words) and (not is_remove_punct or word not in punctuations) ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different variations during tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'be', 'bangalore', '.', 'i', 'be', 'live', 'here', '!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bangalore', '.', 'live', '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With removing stopwords\n",
    "spacy_lemmatizer(\"This is Bangalore. I am living here!\",True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'be', 'bangalore', '.', 'i', 'be', 'live', 'here', '!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this', 'be', 'bangalore', 'i', 'be', 'live', 'here']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With removing punctuations\n",
    "spacy_lemmatizer(\"This is Bangalore. I am living here!\",False,True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'be', 'bangalore', '.', 'i', 'be', 'live', 'here', '!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bangalore', 'live']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With removing stopwords and punctuations\n",
    "spacy_lemmatizer(\"This is Bangalore. I am living here!\",True,True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'be', 'bangalore', '.', 'i', 'be', 'live', 'here', '!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this', 'be', 'bangalore', '.', 'i', 'be', 'live', 'here', '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without removing punctuation and stopwords\n",
    "spacy_lemmatizer(\"This is Bangalore. I am living here!\",verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be removing the stopwords and punctuations since it will be beneficial for seq2seq architecture. The other lemmatization variations with stopwords and punctuations are more useful for simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'bengaluru', '.', 'i', 'am', 'live', 'here', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stemmer(snow_stemmer,\"This is Bengaluru. I am living here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is sufficient especially when corpus is large since it is very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Vectorization\n",
    "This is applied over the tokens.\n",
    "This can be done either by certain dedicated algorithms like Bag of words,TF-IDF or by taking the average word representation in sentence using word representations like Word2Vec,Glove etc..\\\\\n",
    "We will be using word representation since we will be using seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skips UNKNOWN words while generating sentence representation\n",
    "def average_word_rep_sentence_vectorizer(sent,word_vector_obj_list):\n",
    "    res_list = []\n",
    "    for word_vector_obj in word_vector_obj_list:\n",
    "        vector_size = word_vector_obj.vector_size\n",
    "        wv_res = np.zeros(vector_size)\n",
    "        # print(wv_res)\n",
    "        ctr = 1\n",
    "        for w in sent:\n",
    "            if w in word_vector_obj:\n",
    "                ctr += 1\n",
    "                wv_res += word_vector_obj[w]\n",
    "        wv_res = wv_res/ctr\n",
    "    \n",
    "    return wv_res\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32984137 0.38722697 0.17904217 0.77158292 0.37752822] [0.22577246 0.71257047 0.70641463 0.02552281]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71272/2605643107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtst2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtst2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtst1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtst2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "tst1 = np.random.rand(3,5)\n",
    "tst2 = np.random.rand(3,4)\n",
    "print(tst1,tst2)\n",
    "tst = np.stack([tst1,tst2])\n",
    "print(tst,tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/CNN-DailyMail News Dataset/train.csv\n",
      "./data/CNN-DailyMail News Dataset/validation.csv\n",
      "./data/CNN-DailyMail News Dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/CNN-DailyMail News Dataset/train.csv')\n",
    "test_data = pd.read_csv('./data/CNN-DailyMail News Dataset/test.csv')\n",
    "valid_data = pd.read_csv('./data/CNN-DailyMail News Dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237876</th>\n",
       "      <td>bfe46158407b71846f408b141d3276d68bc86665</td>\n",
       "      <td>Football was hit by a fresh racism storm after...</td>\n",
       "      <td>Feyenoord's Europa League match with Roma was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222245</th>\n",
       "      <td>abb2b894628c02b20cddf53c36957532593a9407</td>\n",
       "      <td>By . Associated Press . and David Martosko, U....</td>\n",
       "      <td>Obama appointed three people to the National L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9186</th>\n",
       "      <td>1a082af1a725f85b460b141a76e8d85c9760cf9f</td>\n",
       "      <td>A stunning catalog of torture and the widespre...</td>\n",
       "      <td>U.N. report warns North Korean leader Kim Jong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241992</th>\n",
       "      <td>c52dca6cb45e97628b6f309f70990fca5acabd33</td>\n",
       "      <td>A Florida woman who tried to shoot a snake wit...</td>\n",
       "      <td>April DeMarco, 30, was watching a football pra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154607</th>\n",
       "      <td>53ca5d75870b33ede68cb51340e40fe2ba904733</td>\n",
       "      <td>(CNN) -- Thousands of Ontario, Canada, residen...</td>\n",
       "      <td>NEW: Power restored to 100,000 customers in To...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              id  \\\n",
       "237876  bfe46158407b71846f408b141d3276d68bc86665   \n",
       "222245  abb2b894628c02b20cddf53c36957532593a9407   \n",
       "9186    1a082af1a725f85b460b141a76e8d85c9760cf9f   \n",
       "241992  c52dca6cb45e97628b6f309f70990fca5acabd33   \n",
       "154607  53ca5d75870b33ede68cb51340e40fe2ba904733   \n",
       "\n",
       "                                                  article  \\\n",
       "237876  Football was hit by a fresh racism storm after...   \n",
       "222245  By . Associated Press . and David Martosko, U....   \n",
       "9186    A stunning catalog of torture and the widespre...   \n",
       "241992  A Florida woman who tried to shoot a snake wit...   \n",
       "154607  (CNN) -- Thousands of Ontario, Canada, residen...   \n",
       "\n",
       "                                               highlights  \n",
       "237876  Feyenoord's Europa League match with Roma was ...  \n",
       "222245  Obama appointed three people to the National L...  \n",
       "9186    U.N. report warns North Korean leader Kim Jong...  \n",
       "241992  April DeMarco, 30, was watching a football pra...  \n",
       "154607  NEW: Power restored to 100,000 customers in To...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extra_token_rep(wv,num_samples):\n",
    "    random_indices=np.array(random.sample(range(0,len(wv)), num_samples))\n",
    "    klist = [wv.index_to_key[k] for k in random_indices]\n",
    "    return mean_wordvector(wv,klist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "glove_twitter_200_wv[STRT] = get_extra_token_rep(glove_twitter_200_wv,10)\n",
    "glove_twitter_200_wv[END] = get_extra_token_rep(glove_twitter_200_wv,10)\n",
    "gnews_300_wv[STRT] = get_extra_token_rep(gnews_300_wv,10)\n",
    "gnews_300_wv[END] = get_extra_token_rep(gnews_300_wv,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0001d1afc246a7964130f43ae940af6bc6c57f01\n",
       "article       By . Associated Press . PUBLISHED: . 14:11 EST...\n",
       "highlights    Bishop John Folda, of North Dakota, is taking ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizationDataset(Dataset):\n",
    "    def __init__(self, pandas_frame,src_transform=None,target_transform=None,src_key=\"article\",target_key=\"highlights\"):\n",
    "        self.pandas_frame = pandas_frame\n",
    "        self.src_key = src_key\n",
    "        self.target_key = target_key\n",
    "        self.src_transform = src_transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pandas_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.source_vec,self.target_vec = self.pandas_frame.iloc[idx][self.src_key],self.pandas_frame.iloc[idx][self.target_key]\n",
    "        \n",
    "        if self.src_transform:\n",
    "            self.source_vec = self.src_transform(self.source_vec)\n",
    "        if self.target_transform:\n",
    "            self.target_vec = self.target_transform(self.target_vec)\n",
    "        \n",
    "        return self.source_vec, self.target_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-work-DAG-DNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
